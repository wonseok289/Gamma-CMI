{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9671108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"# Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" # Set the GPU 2 to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d11cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from FeatureAcquisition import FeatureAcquisition\n",
    "from Predictor import Predictor\n",
    "from Generators import GaussianSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from PartialVAE import PartialVAE\n",
    "from metrics_dict import metrics_dict\n",
    "from sklearn.metrics import auc as sklearn_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb8691",
   "metadata": {},
   "source": [
    "# Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c029746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 샘플에서 k개가 관측되었다고 가정\n",
    "# k개의 관측을 임의로 배정\n",
    "\n",
    "def sample_mask_uniform_K_per_sample(bs, d, min_K, max_K): # batch size, feature 개수, 최소 관측 샘플 수, 최대 관측 샘플 수\n",
    "    m = np.zeros((bs, d), dtype=np.float32)\n",
    "    Ks = np.random.randint(min_K, max_K+1, size=(bs,))\n",
    "    for i, K in enumerate(Ks): # Ks의 index와 해당 index의 값\n",
    "        idx = np.random.choice(d, size=K, replace=False)\n",
    "        m[i, idx] = 1.0\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deea2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_mask(mask):\n",
    "    # Subsample by uniformly selecting removal probability, each feature\n",
    "    # has that probability of being removed.\n",
    "    # Multiply by true mask since it may be missing values to begin with.\n",
    "    return (torch.rand_like(mask) > torch.rand_like(mask[:, :1])).float()*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25104c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(numpy_seed, random_seed, torch_seed_cpu, torch_seed_cuda):\n",
    "    random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    np.random.seed(numpy_seed)\n",
    "    torch.manual_seed(torch_seed_cpu)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(torch_seed_cuda)\n",
    "        torch.cuda.manual_seed_all(torch_seed_cuda)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6d22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(\n",
    "    generator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    optimizer_generator,\n",
    "    save_path,\n",
    "    obs_sigma=0.2,\n",
    "    lr_factor=0.2,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,\n",
    "    scheduler_patience=5\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    \n",
    "    scheduler_generator = ReduceLROnPlateau(\n",
    "        optimizer_generator,\n",
    "        mode=\"min\",\n",
    "        factor=lr_factor,\n",
    "        patience=scheduler_patience,\n",
    "        cooldown=cooldown,\n",
    "        min_lr=min_lr,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train \n",
    "        generator.train()\n",
    "        total_loss_generator, total_kl_generator, total_nll_generator = 0.0, 0.0, 0.0\n",
    "        total_train_samples = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            bs = xb.size(0)\n",
    "            \n",
    "            # generator 학습\n",
    "            m_np = subsample_mask(xb)\n",
    "            mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "            loss_generator, logs = generator.loss_func(\n",
    "                xb,\n",
    "                mb,\n",
    "                obs_sigma=obs_sigma,\n",
    "                n_samples=1,\n",
    "            )\n",
    "            \n",
    "            optimizer_generator.zero_grad()\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "            \n",
    "            total_loss_generator += loss_generator.item() * bs\n",
    "            total_kl_generator   += logs[\"KL\"].item() * bs\n",
    "            total_nll_generator  += logs[\"NLL_X\"].item() * bs\n",
    "            \n",
    "            total_train_samples += bs\n",
    "            \n",
    "        train_loss_generator = total_loss_generator / total_train_samples\n",
    "        train_kl   = total_kl_generator / total_train_samples\n",
    "        train_nll  = total_nll_generator / total_train_samples\n",
    "\n",
    "        # Validation \n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            total_metric_generator, total_kl_generator, total_nll_generator = 0.0, 0.0, 0.0\n",
    "            total_val_samples = 0.0\n",
    "            \n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                bs_val = xb.size(0)\n",
    "                \n",
    "                # generator 검증\n",
    "                m_np = subsample_mask(xb)\n",
    "                mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "                val_loss_generator, val_logs = generator.loss_func(\n",
    "                    xb,\n",
    "                    mb,\n",
    "                    obs_sigma=obs_sigma,\n",
    "                    n_samples=1,\n",
    "                )\n",
    "                total_metric_generator += val_loss_generator.item() * bs_val\n",
    "                total_kl_generator += val_logs[\"KL\"].item() * bs_val\n",
    "                total_nll_generator += val_logs[\"NLL_X\"].item() * bs_val\n",
    "                \n",
    "                total_val_samples += bs_val\n",
    "            \n",
    "            val_metric_generator = total_metric_generator / total_val_samples\n",
    "            val_kl = total_kl_generator / total_val_samples\n",
    "            val_nll = total_nll_generator / total_val_samples\n",
    "            \n",
    "            scheduler_generator.step(val_metric_generator)\n",
    "            if val_metric_generator == scheduler_generator.best:\n",
    "                best_state = copy.deepcopy(generator.state_dict())\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(best_state, os.path.join(save_path, \"best_model_generator.pt\"))\n",
    "    \n",
    "        # 로그 출력\n",
    "        current_lr_generator = optimizer_generator.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"train_loss_generator={train_loss_generator:.4f} | \"\n",
    "            f\"train_kl={train_kl:.4f} | \"\n",
    "            f\"train_nll={train_nll:.4f} | \"\n",
    "            f\"val_metric_generator={val_metric_generator:.4f} | \"\n",
    "            f\"val_kl={val_kl:.4f} | \"\n",
    "            f\"val_nll={val_nll:.4f} | \"\n",
    "            f\"best={scheduler_generator.best:.4f} | \"\n",
    "            f\"lr_generator={current_lr_generator:.6f}\"\n",
    "        )\n",
    "        \n",
    "    print(f\"\\nTraining complete. Best validation metric: {scheduler_generator.best:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abb53d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_acquisition(\n",
    "    predictor,\n",
    "    generator,\n",
    "    val_loader,\n",
    "    metric_f,\n",
    "    num_samples=10,\n",
    "    alpha=1.0,\n",
    "    gamma=0.5\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predictor.eval()\n",
    "    val_auc=0\n",
    "\n",
    "    accs = []\n",
    "    for xb, yb in val_loader:\n",
    "        m_np = np.zeros_like(xb)\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        \n",
    "        mv = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "        initial_acc = metric_f(predictor(xb, mv), yb)\n",
    "        val_metrics = [initial_acc]\n",
    "        print(f\"Step 0/{xb.shape[1]} (no features) | Acc: {initial_acc:.4f}\")\n",
    "\n",
    "        for t in range(1, xb.shape[1]+1):\n",
    "            FA = FeatureAcquisition(\n",
    "                x=xb,\n",
    "                m=mv,\n",
    "                generative_model=generator,\n",
    "                num_samples=num_samples,\n",
    "                predictor=predictor,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma\n",
    "            )\n",
    "\n",
    "            # mask 업데이트\n",
    "            mv, _ = FA.acquire()\n",
    "            step_acc = metric_f(predictor(xb, mv), yb)\n",
    "            val_metrics.append(step_acc)\n",
    "            print(f\"Step {t}/{xb.shape[1]} | Acc: {step_acc:.4f}\")\n",
    "\n",
    "        val_metrics = np.array(val_metrics)\n",
    "        val_auc += sklearn_auc(np.arange(xb.shape[1]+1), val_metrics)/(len(val_loader)*xb.shape[1])\n",
    "        \n",
    "    return val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictor(\n",
    "    predictor,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    optimizer_predictor,\n",
    "    criterion,\n",
    "    metric_f,\n",
    "    save_path,\n",
    "    lr_factor=0.2,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,\n",
    "    scheduler_patience=5\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predictor.to(device)\n",
    "    generator.to(device)\n",
    "\n",
    "    scheduler_predictor = ReduceLROnPlateau(\n",
    "        optimizer_predictor,\n",
    "        mode=\"max\",\n",
    "        factor=lr_factor,\n",
    "        patience=scheduler_patience,\n",
    "        cooldown=cooldown,\n",
    "        min_lr=min_lr,\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train \n",
    "        predictor.train()\n",
    "        total_loss_predictor = 0.0\n",
    "        total_train_samples = 0.0\n",
    "        \n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            bs = xb.size(0)\n",
    "\n",
    "            # predictor 학습\n",
    "            m_np = subsample_mask(xb)\n",
    "            mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "\n",
    "            logits = predictor(xb, mb)\n",
    "            loss_predictor = criterion(logits, yb)\n",
    "\n",
    "            optimizer_predictor.zero_grad()\n",
    "            loss_predictor.backward()\n",
    "            optimizer_predictor.step()\n",
    "\n",
    "            total_loss_predictor += loss_predictor.item() * bs\n",
    "            \n",
    "            total_train_samples += bs\n",
    "          \n",
    "            \n",
    "        train_loss_predictor = total_loss_predictor / total_train_samples\n",
    "\n",
    "        # Validation \n",
    "        predictor.eval()\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            total_metric_predictor = 0.0\n",
    "            total_val_samples = 0.0\n",
    "            \n",
    "            for xb, yb in val_loader:\n",
    "                xb=xb.to(device)\n",
    "                yb=yb.to(device)\n",
    "                bs_val = xb.size(0)\n",
    "                \n",
    "                # predictor 검증\n",
    "                m_np = subsample_mask(xb)\n",
    "                mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "                total_metric_predictor += metric_f(predictor(xb, mb), yb) * bs_val\n",
    "                \n",
    "                \n",
    "                total_val_samples += bs_val\n",
    "            \n",
    "            val_metric_predictor = total_metric_predictor / total_val_samples\n",
    "            val_auc = run_feature_acquisition(predictor, generator, val_loader, metric_f)\n",
    "\n",
    "            scheduler_predictor.step(val_auc)\n",
    "            if val_auc == scheduler_predictor.best:\n",
    "                best_state = copy.deepcopy(predictor.state_dict())\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(best_state, os.path.join(save_path,\"best_model_predictor.pt\"))\n",
    "\n",
    "    \n",
    "        # 로그 출력\n",
    "        current_lr_predictor = optimizer_predictor.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"train_loss_predictor={train_loss_predictor:.4f} | \"\n",
    "            f\"Val Accuracy: {val_auc:.4f}|{scheduler_predictor.best:.4f}, Val Metric_predictor: {val_metric_predictor:.4f} | \"\n",
    "            f\"lr_predictor={current_lr_predictor:.6f}\"\n",
    "        )\n",
    "        \n",
    "    val_auc = run_feature_acquisition(predictor, generator, val_loader, metric_f)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(val_auc, os.path.join(save_path, \"val_auc.pt\"))\n",
    "    print(f\"\\nTraining complete, Zero Acquisition AUC: {val_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831333e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_generator(\n",
    "#     generator,\n",
    "#     train_loader,\n",
    "#     X_val,\n",
    "#     D, # feature 개수\n",
    "#     epochs,\n",
    "#     optimizer,\n",
    "#     obs_sigma=0.2,\n",
    "#     lr_factor=0.2,\n",
    "#     cooldown=0,\n",
    "#     min_lr=1e-7,\n",
    "#     scheduler_patience=5\n",
    "# ):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     generator.to(device)\n",
    "#     X_val = X_val.to(device)\n",
    "\n",
    "#     scheduler = ReduceLROnPlateau(\n",
    "#         optimizer,\n",
    "#         mode=\"min\", # loss 기준이기 때문 \n",
    "#         factor=lr_factor,\n",
    "#         patience=scheduler_patience,\n",
    "#         cooldown=cooldown,\n",
    "#         min_lr=min_lr,\n",
    "#     )\n",
    "\n",
    "#     best_val_loss = float(\"inf\")\n",
    "#     best_state = None\n",
    "\n",
    "#     for ep in range(epochs):\n",
    "#         # Train\n",
    "#         generator.train()\n",
    "#         total_loss, total_kl, total_nll = 0.0, 0.0, 0.0\n",
    "#         count = 0\n",
    "\n",
    "#         for xb, _ in train_loader:\n",
    "#             xb = xb.to(device).float()\n",
    "#             bs = xb.size(0)\n",
    "\n",
    "#             m_np = sample_mask_uniform_K_per_sample(\n",
    "#                 bs=bs,\n",
    "#                 d=D,\n",
    "#                 min_K=1,\n",
    "#                 max_K=D,\n",
    "#             )\n",
    "#             mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "\n",
    "#             loss, logs = generator.loss_func(\n",
    "#                 xb,\n",
    "#                 mb,\n",
    "#                 obs_sigma=obs_sigma,\n",
    "#                 n_samples=1,\n",
    "#             )\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item() * bs\n",
    "#             total_kl   += logs[\"KL\"].item() * bs\n",
    "#             total_nll  += logs[\"NLL_X\"].item() * bs\n",
    "#             count      += bs\n",
    "\n",
    "#         train_loss = total_loss / count\n",
    "#         train_kl   = total_kl / count\n",
    "#         train_nll  = total_nll / count\n",
    "\n",
    "#         # Validation\n",
    "#         generator.eval()\n",
    "#         with torch.no_grad():\n",
    "#             bs_val = X_val.size(0)\n",
    "#             m_np = sample_mask_uniform_K_per_sample(\n",
    "#                 bs=bs_val,\n",
    "#                 d=D,\n",
    "#                 min_K=1,\n",
    "#                 max_K=D,\n",
    "#             )\n",
    "#             mv = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
    "\n",
    "#             val_loss_tensor, val_logs = generator.loss_func(\n",
    "#                 X_val,\n",
    "#                 mv,\n",
    "#                 obs_sigma=obs_sigma,\n",
    "#                 n_samples=1,\n",
    "#             )\n",
    "\n",
    "#             val_loss = val_loss_tensor.item()\n",
    "#             val_kl   = val_logs[\"KL\"].item()\n",
    "#             val_nll  = val_logs[\"NLL_X\"].item()\n",
    "\n",
    "#         # 스케줄러 업데이트 (val_loss 기준)\n",
    "#         scheduler.step(val_loss)\n",
    "#         current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "#         print(\n",
    "#             f\"[PVAE ep {ep:02d}] \"\n",
    "#             f\"train_loss={train_loss:.4f}  train_KL={train_kl:.4f}  train_NLL_X={train_nll:.4f} | \"\n",
    "#             f\"val_loss={val_loss:.4f}  val_KL={val_kl:.4f}  val_NLL_X={val_nll:.4f} | \"\n",
    "#             f\"lr={current_lr:.6f}\"\n",
    "#         )\n",
    "\n",
    "#         # best model 저장 (val_loss 최소 기준)\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             best_state = copy.deepcopy(generator.state_dict())\n",
    "\n",
    "#     # 가장 좋은 val_loss 기준으로 weight 복원\n",
    "#     if best_state is not None:\n",
    "#         generator.load_state_dict(best_state)\n",
    "\n",
    "#     print(f\"Best val_loss = {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d13c9",
   "metadata": {},
   "source": [
    "# CUBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d61e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\", \"cube\")\n",
    "\n",
    "X_train = torch.load(f\"{DATA_DIR}/X_train_cdf.pt\").float()\n",
    "y_train = torch.load(f\"{DATA_DIR}/y_train.pt\").long()\n",
    "\n",
    "X_val   = torch.load(f\"{DATA_DIR}/X_val_cdf.pt\").float()\n",
    "y_val   = torch.load(f\"{DATA_DIR}/y_val.pt\").long()\n",
    "\n",
    "X_test = torch.load(f\"{DATA_DIR}/X_test_cdf.pt\").float()\n",
    "y_test = torch.load(f\"{DATA_DIR}/y_test.pt\").long()\n",
    "\n",
    "dataset_dict = torch.load(f\"{DATA_DIR}/dataset_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47647f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/716532247.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/716532247.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | train_loss_generator=-3640114.2894 | train_kl=72.5124 | train_nll=-3640186.8151 | val_metric_generator=-6978588.5000 | val_kl=130.2080 | val_nll=-6978718.5000 | best=-6978588.5000 | lr_generator=0.001000\n",
      "Epoch 2/100 | train_loss_generator=-50671350.8781 | train_kl=139.5342 | train_nll=-50671490.6698 | val_metric_generator=-92152432.0000 | val_kl=141.8552 | val_nll=-92152576.0000 | best=-92152432.0000 | lr_generator=0.001000\n",
      "Epoch 3/100 | train_loss_generator=-247260958.8395 | train_kl=144.5202 | train_nll=-247261100.1347 | val_metric_generator=-57473412.0000 | val_kl=152.7946 | val_nll=-57473564.0000 | best=-92152432.0000 | lr_generator=0.001000\n",
      "Epoch 4/100 | train_loss_generator=-811743491.6864 | train_kl=149.0298 | train_nll=-811743634.3509 | val_metric_generator=-62460293120.0000 | val_kl=152.8662 | val_nll=-62460293120.0000 | best=-62460293120.0000 | lr_generator=0.001000\n",
      "Epoch 5/100 | train_loss_generator=-1781372273.7237 | train_kl=150.1601 | train_nll=-1781372453.4699 | val_metric_generator=-149278228480.0000 | val_kl=153.1144 | val_nll=-149278228480.0000 | best=-149278228480.0000 | lr_generator=0.001000\n",
      "Epoch 6/100 | train_loss_generator=-3061550895.8080 | train_kl=153.8856 | train_nll=-3061550985.6768 | val_metric_generator=-470143959040.0000 | val_kl=152.8510 | val_nll=-470143959040.0000 | best=-470143959040.0000 | lr_generator=0.001000\n",
      "Epoch 7/100 | train_loss_generator=-5425468869.6661 | train_kl=152.5869 | train_nll=-5425468944.4011 | val_metric_generator=-834320531456.0000 | val_kl=152.6436 | val_nll=-834320531456.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 8/100 | train_loss_generator=-9786248605.8283 | train_kl=162.1975 | train_nll=-9786248673.0581 | val_metric_generator=-744744878080.0000 | val_kl=176.9747 | val_nll=-744744878080.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 9/100 | train_loss_generator=-14032281946.2144 | train_kl=165.4962 | train_nll=-14032282017.2459 | val_metric_generator=-57303318528.0000 | val_kl=162.5377 | val_nll=-57303318528.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 10/100 | train_loss_generator=-19108766791.6800 | train_kl=159.9810 | train_nll=-19108766871.4155 | val_metric_generator=-24700729344.0000 | val_kl=163.5714 | val_nll=-24700729344.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 11/100 | train_loss_generator=-25859692051.1488 | train_kl=162.3624 | train_nll=-25859692121.0368 | val_metric_generator=-22284693504.0000 | val_kl=168.3118 | val_nll=-22284693504.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 12/100 | train_loss_generator=-34712223429.4613 | train_kl=172.2804 | train_nll=-34712223497.6256 | val_metric_generator=-58191298560.0000 | val_kl=177.1551 | val_nll=-58191298560.0000 | best=-834320531456.0000 | lr_generator=0.001000\n",
      "Epoch 13/100 | train_loss_generator=-49313436877.9605 | train_kl=172.9889 | train_nll=-49313436936.5675 | val_metric_generator=-8237801472.0000 | val_kl=172.7007 | val_nll=-8237801472.0000 | best=-834320531456.0000 | lr_generator=0.000200\n",
      "Epoch 14/100 | train_loss_generator=-52345116720.0256 | train_kl=171.4491 | train_nll=-52345116790.3744 | val_metric_generator=-42506006528.0000 | val_kl=173.6291 | val_nll=-42506006528.0000 | best=-834320531456.0000 | lr_generator=0.000200\n",
      "Epoch 15/100 | train_loss_generator=-59676225929.7621 | train_kl=172.8976 | train_nll=-59676225980.2795 | val_metric_generator=-28040648704.0000 | val_kl=174.8730 | val_nll=-28040648704.0000 | best=-834320531456.0000 | lr_generator=0.000200\n",
      "Epoch 16/100 | train_loss_generator=-56473396841.8133 | train_kl=171.7791 | train_nll=-56473396898.7477 | val_metric_generator=-2898990727168.0000 | val_kl=169.3420 | val_nll=-2898990727168.0000 | best=-2898990727168.0000 | lr_generator=0.000200\n",
      "Epoch 17/100 | train_loss_generator=-64285001765.8197 | train_kl=171.4585 | train_nll=-64285001824.3925 | val_metric_generator=-4169842819072.0000 | val_kl=172.5646 | val_nll=-4169842819072.0000 | best=-4169842819072.0000 | lr_generator=0.000200\n",
      "Epoch 18/100 | train_loss_generator=-63561645655.3813 | train_kl=173.6366 | train_nll=-63561645718.9376 | val_metric_generator=-51002904576.0000 | val_kl=174.4858 | val_nll=-51002904576.0000 | best=-4169842819072.0000 | lr_generator=0.000200\n",
      "Epoch 19/100 | train_loss_generator=-70921050924.6464 | train_kl=174.4854 | train_nll=-70921050974.3445 | val_metric_generator=-8199363100672.0000 | val_kl=172.9331 | val_nll=-8199363100672.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 20/100 | train_loss_generator=-62812568103.5947 | train_kl=173.2987 | train_nll=-62812568160.5973 | val_metric_generator=-7209090023424.0000 | val_kl=172.3360 | val_nll=-7209090023424.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 21/100 | train_loss_generator=-69119175622.8608 | train_kl=172.8572 | train_nll=-69119175688.0555 | val_metric_generator=-5176864800768.0000 | val_kl=172.6109 | val_nll=-5176864800768.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 22/100 | train_loss_generator=-74917883960.7979 | train_kl=172.0013 | train_nll=-74917884012.6805 | val_metric_generator=-21770186752.0000 | val_kl=173.6815 | val_nll=-21770186752.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 23/100 | train_loss_generator=-79714877014.2891 | train_kl=174.3522 | train_nll=-79714877065.0795 | val_metric_generator=-2603630460928.0000 | val_kl=177.8469 | val_nll=-2603630460928.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 24/100 | train_loss_generator=-79952639331.5328 | train_kl=175.3866 | train_nll=-79952639379.5925 | val_metric_generator=-161985445888.0000 | val_kl=176.5932 | val_nll=-161985445888.0000 | best=-8199363100672.0000 | lr_generator=0.000200\n",
      "Epoch 25/100 | train_loss_generator=-86329782088.7723 | train_kl=174.3200 | train_nll=-86329782133.2821 | val_metric_generator=-8341028864.0000 | val_kl=173.5516 | val_nll=-8341028864.0000 | best=-8199363100672.0000 | lr_generator=0.000040\n",
      "Epoch 26/100 | train_loss_generator=-81216831323.3408 | train_kl=174.7328 | train_nll=-81216831385.8731 | val_metric_generator=-3331014524928.0000 | val_kl=175.9196 | val_nll=-3331014524928.0000 | best=-8199363100672.0000 | lr_generator=0.000040\n",
      "Epoch 27/100 | train_loss_generator=-84311138871.2960 | train_kl=175.5973 | train_nll=-84311138926.6603 | val_metric_generator=-11228776759296.0000 | val_kl=176.4631 | val_nll=-11228776759296.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 28/100 | train_loss_generator=-82838411311.7867 | train_kl=175.7870 | train_nll=-82838411363.9424 | val_metric_generator=-63906402304.0000 | val_kl=176.2655 | val_nll=-63906402304.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 29/100 | train_loss_generator=-89886546312.1237 | train_kl=176.3799 | train_nll=-89886546363.7333 | val_metric_generator=-4522842259456.0000 | val_kl=177.1446 | val_nll=-4522842259456.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 30/100 | train_loss_generator=-84437493327.4624 | train_kl=176.6113 | train_nll=-84437493386.7179 | val_metric_generator=-2113368489984.0000 | val_kl=177.1390 | val_nll=-2113368489984.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 31/100 | train_loss_generator=-83727091987.6608 | train_kl=177.2678 | train_nll=-83727092038.0416 | val_metric_generator=-4281532416000.0000 | val_kl=177.8823 | val_nll=-4281532416000.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 32/100 | train_loss_generator=-84265480076.0832 | train_kl=177.5637 | train_nll=-84265480123.7333 | val_metric_generator=-18218319872.0000 | val_kl=178.0327 | val_nll=-18218319872.0000 | best=-11228776759296.0000 | lr_generator=0.000040\n",
      "Epoch 33/100 | train_loss_generator=-87616728198.5195 | train_kl=177.6764 | train_nll=-87616728249.8901 | val_metric_generator=-127750823936.0000 | val_kl=177.8839 | val_nll=-127750823936.0000 | best=-11228776759296.0000 | lr_generator=0.000008\n",
      "Epoch 34/100 | train_loss_generator=-87011050794.7349 | train_kl=177.6371 | train_nll=-87011050852.6251 | val_metric_generator=-3882024173568.0000 | val_kl=177.9668 | val_nll=-3882024173568.0000 | best=-11228776759296.0000 | lr_generator=0.000008\n",
      "Epoch 35/100 | train_loss_generator=-77523610617.9243 | train_kl=177.7277 | train_nll=-77523610671.4795 | val_metric_generator=-11468877594624.0000 | val_kl=178.2835 | val_nll=-11468877594624.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 36/100 | train_loss_generator=-84290182791.9872 | train_kl=177.8096 | train_nll=-84290182851.7888 | val_metric_generator=-9694663933952.0000 | val_kl=177.9422 | val_nll=-9694663933952.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 37/100 | train_loss_generator=-87068635743.3003 | train_kl=177.6126 | train_nll=-87068635799.5861 | val_metric_generator=-138929111040.0000 | val_kl=178.4296 | val_nll=-138929111040.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 38/100 | train_loss_generator=-82112703435.8784 | train_kl=177.6894 | train_nll=-82112703491.6181 | val_metric_generator=-27566002176.0000 | val_kl=178.5122 | val_nll=-27566002176.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 39/100 | train_loss_generator=-85095697530.8800 | train_kl=177.9418 | train_nll=-85095697576.2091 | val_metric_generator=-18944958464.0000 | val_kl=178.2526 | val_nll=-18944958464.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 40/100 | train_loss_generator=-87575124232.3285 | train_kl=178.0786 | train_nll=-87575124284.7573 | val_metric_generator=-33294002176.0000 | val_kl=178.4498 | val_nll=-33294002176.0000 | best=-11468877594624.0000 | lr_generator=0.000008\n",
      "Epoch 41/100 | train_loss_generator=-82954279801.6512 | train_kl=177.9477 | train_nll=-82954279851.8955 | val_metric_generator=-9305364480.0000 | val_kl=178.2775 | val_nll=-9305364480.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 42/100 | train_loss_generator=-86362831993.7877 | train_kl=178.0245 | train_nll=-86362832039.9360 | val_metric_generator=-15208960000.0000 | val_kl=178.0315 | val_nll=-15208960000.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 43/100 | train_loss_generator=-82102985940.9920 | train_kl=178.0581 | train_nll=-82102985997.5168 | val_metric_generator=-39889108992.0000 | val_kl=178.6889 | val_nll=-39889108992.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 44/100 | train_loss_generator=-82219332141.4656 | train_kl=177.9887 | train_nll=-82219332191.9147 | val_metric_generator=-11404622954496.0000 | val_kl=178.6698 | val_nll=-11404622954496.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 45/100 | train_loss_generator=-86122256326.6560 | train_kl=178.1732 | train_nll=-86122256377.4464 | val_metric_generator=-53805568000.0000 | val_kl=178.4228 | val_nll=-53805568000.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 46/100 | train_loss_generator=-93936317721.7365 | train_kl=178.1640 | train_nll=-93936317760.7168 | val_metric_generator=-11334111461376.0000 | val_kl=178.2780 | val_nll=-11334111461376.0000 | best=-11468877594624.0000 | lr_generator=0.000002\n",
      "Epoch 47/100 | train_loss_generator=-86336350195.0293 | train_kl=178.2349 | train_nll=-86336350255.6843 | val_metric_generator=-47723032576.0000 | val_kl=178.6197 | val_nll=-47723032576.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 48/100 | train_loss_generator=-87957666846.3104 | train_kl=178.2624 | train_nll=-87957666892.3221 | val_metric_generator=-29156243456.0000 | val_kl=178.5768 | val_nll=-29156243456.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 49/100 | train_loss_generator=-88583959861.1115 | train_kl=178.3378 | train_nll=-88583959914.6325 | val_metric_generator=-52793040896.0000 | val_kl=178.5939 | val_nll=-52793040896.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 50/100 | train_loss_generator=-88901272841.4208 | train_kl=178.1779 | train_nll=-88901272884.1557 | val_metric_generator=-5673126461440.0000 | val_kl=178.6563 | val_nll=-5673126461440.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 51/100 | train_loss_generator=-83212832561.4251 | train_kl=178.2180 | train_nll=-83212832608.2560 | val_metric_generator=-19548917760.0000 | val_kl=178.4049 | val_nll=-19548917760.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 52/100 | train_loss_generator=-90266287235.6864 | train_kl=178.2881 | train_nll=-90266287291.0507 | val_metric_generator=-5723415117824.0000 | val_kl=178.6304 | val_nll=-5723415117824.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 53/100 | train_loss_generator=-81049163200.1024 | train_kl=178.0965 | train_nll=-81049163257.1733 | val_metric_generator=-14432993280.0000 | val_kl=178.8513 | val_nll=-14432993280.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 54/100 | train_loss_generator=-91572346045.5083 | train_kl=178.1094 | train_nll=-91572346090.2912 | val_metric_generator=-12268259328.0000 | val_kl=178.7814 | val_nll=-12268259328.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 55/100 | train_loss_generator=-86077897041.6469 | train_kl=178.1933 | train_nll=-86077897086.9077 | val_metric_generator=-8681485762560.0000 | val_kl=178.2672 | val_nll=-8681485762560.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 56/100 | train_loss_generator=-86220601909.2480 | train_kl=178.2652 | train_nll=-86220601960.0384 | val_metric_generator=-4961537622016.0000 | val_kl=178.4575 | val_nll=-4961537622016.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 57/100 | train_loss_generator=-90816605638.3829 | train_kl=178.1439 | train_nll=-90816605685.8965 | val_metric_generator=-183874977792.0000 | val_kl=178.7571 | val_nll=-183874977792.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 58/100 | train_loss_generator=-84556108787.9851 | train_kl=178.3702 | train_nll=-84556108844.7829 | val_metric_generator=-60529913856.0000 | val_kl=178.5395 | val_nll=-60529913856.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 59/100 | train_loss_generator=-89712772145.1520 | train_kl=178.1773 | train_nll=-89712772189.3888 | val_metric_generator=-26004905984.0000 | val_kl=178.8384 | val_nll=-26004905984.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 60/100 | train_loss_generator=-90285191462.3317 | train_kl=178.1455 | train_nll=-90285191515.8869 | val_metric_generator=-24876509184.0000 | val_kl=178.4548 | val_nll=-24876509184.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 61/100 | train_loss_generator=-81201126666.7861 | train_kl=178.2599 | train_nll=-81201126719.4880 | val_metric_generator=-5844565491712.0000 | val_kl=178.5132 | val_nll=-5844565491712.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 62/100 | train_loss_generator=-86200173175.0571 | train_kl=178.1581 | train_nll=-86200173230.7627 | val_metric_generator=-4148990836736.0000 | val_kl=178.5881 | val_nll=-4148990836736.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 63/100 | train_loss_generator=-85326348744.0213 | train_kl=178.3251 | train_nll=-85326348797.2011 | val_metric_generator=-67748532224.0000 | val_kl=178.3540 | val_nll=-67748532224.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 64/100 | train_loss_generator=-95340648322.3893 | train_kl=178.2916 | train_nll=-95340648365.2608 | val_metric_generator=-10862597242880.0000 | val_kl=179.0767 | val_nll=-10862597242880.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 65/100 | train_loss_generator=-86160890087.5605 | train_kl=178.2808 | train_nll=-86160890135.6203 | val_metric_generator=-4003745024.0000 | val_kl=178.7171 | val_nll=-4003745280.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 66/100 | train_loss_generator=-92123102979.6864 | train_kl=178.4019 | train_nll=-92123103031.8421 | val_metric_generator=-18887407616.0000 | val_kl=178.6291 | val_nll=-18887407616.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 67/100 | train_loss_generator=-84337846529.7749 | train_kl=178.0812 | train_nll=-84337846570.1888 | val_metric_generator=-5229310377984.0000 | val_kl=178.2141 | val_nll=-5229310377984.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 68/100 | train_loss_generator=-88105431286.3061 | train_kl=178.1894 | train_nll=-88105431337.9157 | val_metric_generator=-21369790464.0000 | val_kl=178.8891 | val_nll=-21369790464.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 69/100 | train_loss_generator=-90699781320.1579 | train_kl=178.3241 | train_nll=-90699781359.7525 | val_metric_generator=-10847191564288.0000 | val_kl=178.6677 | val_nll=-10847191564288.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 70/100 | train_loss_generator=-88613605808.5376 | train_kl=178.3530 | train_nll=-88613605862.1952 | val_metric_generator=-58062098432.0000 | val_kl=178.5084 | val_nll=-58062098432.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 71/100 | train_loss_generator=-97801410736.9472 | train_kl=178.3928 | train_nll=-97801410776.8149 | val_metric_generator=-29529747456.0000 | val_kl=178.5278 | val_nll=-29529747456.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 72/100 | train_loss_generator=-83634894840.6272 | train_kl=178.2724 | train_nll=-83634894893.3291 | val_metric_generator=-31090835456.0000 | val_kl=178.2825 | val_nll=-31090835456.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 73/100 | train_loss_generator=-79644174248.6187 | train_kl=178.3870 | train_nll=-79644174317.4315 | val_metric_generator=-16450062336.0000 | val_kl=178.7009 | val_nll=-16450062336.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 74/100 | train_loss_generator=-85868437280.0853 | train_kl=178.0657 | train_nll=-85868437331.4219 | val_metric_generator=-27932219392.0000 | val_kl=178.6521 | val_nll=-27932219392.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 75/100 | train_loss_generator=-88371319604.5653 | train_kl=178.1879 | train_nll=-88371319647.9829 | val_metric_generator=-59086516224.0000 | val_kl=178.8248 | val_nll=-59086516224.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 76/100 | train_loss_generator=-87889663428.7445 | train_kl=178.3854 | train_nll=-87889663484.7232 | val_metric_generator=-57937272832.0000 | val_kl=178.6593 | val_nll=-57937272832.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 77/100 | train_loss_generator=-92281792106.7691 | train_kl=178.2850 | train_nll=-92281792158.6517 | val_metric_generator=-88967872512.0000 | val_kl=178.8315 | val_nll=-88967872512.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 78/100 | train_loss_generator=-87719645245.1669 | train_kl=178.3469 | train_nll=-87719645293.7728 | val_metric_generator=-56853958656.0000 | val_kl=178.5907 | val_nll=-56853958656.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 79/100 | train_loss_generator=-93814197846.2891 | train_kl=178.2117 | train_nll=-93814197895.1680 | val_metric_generator=-6207176704000.0000 | val_kl=178.6143 | val_nll=-6207176704000.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 80/100 | train_loss_generator=-85934409203.7120 | train_kl=178.0826 | train_nll=-85934409250.4064 | val_metric_generator=-10568329068544.0000 | val_kl=178.2262 | val_nll=-10568329068544.0000 | best=-11468877594624.0000 | lr_generator=0.000000\n",
      "Epoch 81/100 | train_loss_generator=-89466216258.6965 | train_kl=178.2612 | train_nll=-89466216311.8763 | val_metric_generator=-13191909212160.0000 | val_kl=178.4723 | val_nll=-13191909212160.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 82/100 | train_loss_generator=-85910576685.9435 | train_kl=178.2007 | train_nll=-85910576744.6869 | val_metric_generator=-37755695104.0000 | val_kl=178.5956 | val_nll=-37755695104.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 83/100 | train_loss_generator=-92433869146.5216 | train_kl=178.1961 | train_nll=-92433869194.9909 | val_metric_generator=-40410718208.0000 | val_kl=178.3492 | val_nll=-40410718208.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 84/100 | train_loss_generator=-81225267082.0352 | train_kl=178.3996 | train_nll=-81225267132.5525 | val_metric_generator=-30263810048.0000 | val_kl=178.4810 | val_nll=-30263810048.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 85/100 | train_loss_generator=-88628325699.8571 | train_kl=178.2767 | train_nll=-88628325755.0165 | val_metric_generator=-8090408189952.0000 | val_kl=178.6688 | val_nll=-8090408189952.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 86/100 | train_loss_generator=-92357768627.6779 | train_kl=178.1633 | train_nll=-92357768684.4757 | val_metric_generator=-8250992361472.0000 | val_kl=178.5930 | val_nll=-8250992361472.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 87/100 | train_loss_generator=-81055364558.0971 | train_kl=178.0646 | train_nll=-81055364611.5499 | val_metric_generator=-6997062713344.0000 | val_kl=178.6402 | val_nll=-6997062713344.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 88/100 | train_loss_generator=-90941082357.7600 | train_kl=178.3097 | train_nll=-90941082405.4101 | val_metric_generator=-24501575680.0000 | val_kl=178.2499 | val_nll=-24501575680.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 89/100 | train_loss_generator=-91467839442.3979 | train_kl=178.3792 | train_nll=-91467839484.7232 | val_metric_generator=-89620783104.0000 | val_kl=178.6935 | val_nll=-89620783104.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 90/100 | train_loss_generator=-86905163811.4987 | train_kl=178.3922 | train_nll=-86905163863.3813 | val_metric_generator=-67289829376.0000 | val_kl=178.4516 | val_nll=-67289829376.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 91/100 | train_loss_generator=-83581472517.0517 | train_kl=178.3193 | train_nll=-83581472571.6651 | val_metric_generator=-104999337984.0000 | val_kl=178.6040 | val_nll=-104999337984.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 92/100 | train_loss_generator=-85582869590.6987 | train_kl=178.2967 | train_nll=-85582869640.2603 | val_metric_generator=-14385872896.0000 | val_kl=178.4547 | val_nll=-14385872896.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 93/100 | train_loss_generator=-83739711305.3184 | train_kl=178.1688 | train_nll=-83739711352.8320 | val_metric_generator=-20019703808.0000 | val_kl=178.7018 | val_nll=-20019703808.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 94/100 | train_loss_generator=-96690878343.9872 | train_kl=178.3917 | train_nll=-96690878391.3643 | val_metric_generator=-28797298688.0000 | val_kl=178.6731 | val_nll=-28797298688.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 95/100 | train_loss_generator=-88534447592.2432 | train_kl=178.2838 | train_nll=-88534447641.6683 | val_metric_generator=-48600662016.0000 | val_kl=178.7858 | val_nll=-48600662016.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 96/100 | train_loss_generator=-83570019408.5547 | train_kl=178.3285 | train_nll=-83570019462.8949 | val_metric_generator=-19842744320.0000 | val_kl=179.0880 | val_nll=-19842744320.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 97/100 | train_loss_generator=-89383079903.5051 | train_kl=178.2891 | train_nll=-89383079955.3877 | val_metric_generator=-4964562763776.0000 | val_kl=178.5466 | val_nll=-4964562763776.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 98/100 | train_loss_generator=-84009468944.6571 | train_kl=178.2506 | train_nll=-84009468999.6800 | val_metric_generator=-5052408791040.0000 | val_kl=178.4835 | val_nll=-5052408791040.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 99/100 | train_loss_generator=-86041625598.6347 | train_kl=178.2637 | train_nll=-86041625640.8235 | val_metric_generator=-8163761324032.0000 | val_kl=178.4510 | val_nll=-8163761324032.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "Epoch 100/100 | train_loss_generator=-82905660467.0635 | train_kl=178.3891 | train_nll=-82905660515.8741 | val_metric_generator=-128710803456.0000 | val_kl=178.6685 | val_nll=-128710803456.0000 | best=-13191909212160.0000 | lr_generator=0.000000\n",
      "\n",
      "Training complete. Best validation metric: -13191909212160.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "dataset_name = dataset_dict[\"dataset\"]\n",
    "num_con_features = dataset_dict[\"num_con_features\"]\n",
    "num_cat_features = dataset_dict[\"num_cat_features\"]\n",
    "num_classes = dataset_dict[\"out_dim\"]\n",
    "save_path = os.path.join(ROOT_DIR, \"saved_models\", dataset_name)\n",
    "\n",
    "\n",
    "pvae = PartialVAE(\n",
    "    input_type=\"continuous\",\n",
    "    num_con_features=num_con_features,\n",
    "    num_cat_features=num_cat_features,\n",
    "    hidden_dim_con=30,\n",
    "    most_categories=max(1, 0),   # 내부 차원 계산을 위해 최소 1\n",
    "    c_dim=16,\n",
    "    hid_enc=100,\n",
    "    hid_dec=100,\n",
    "    latent_dim=30\n",
    ")\n",
    "optimizer_pvae = AdamW(pvae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_ds, batch_size=len(X_val), shuffle=False, drop_last=False)\n",
    "\n",
    "train_generator(\n",
    "    generator=pvae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs,\n",
    "    optimizer_generator=optimizer_pvae,\n",
    "    save_path=save_path,\n",
    "    obs_sigma=0.2,\n",
    "    lr_factor=0.2,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,\n",
    "    scheduler_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7b1cd",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de5fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1218\n",
      "Step 2/20 | Acc: 0.1306\n",
      "Step 3/20 | Acc: 0.1412\n",
      "Step 4/20 | Acc: 0.1414\n",
      "Step 5/20 | Acc: 0.1411\n",
      "Step 6/20 | Acc: 0.1463\n",
      "Step 7/20 | Acc: 0.1478\n",
      "Step 8/20 | Acc: 0.1522\n",
      "Step 9/20 | Acc: 0.1564\n",
      "Step 10/20 | Acc: 0.1541\n",
      "Step 11/20 | Acc: 0.1536\n",
      "Step 12/20 | Acc: 0.1507\n",
      "Step 13/20 | Acc: 0.1506\n",
      "Step 14/20 | Acc: 0.1489\n",
      "Step 15/20 | Acc: 0.1480\n",
      "Step 16/20 | Acc: 0.1456\n",
      "Step 17/20 | Acc: 0.1436\n",
      "Step 18/20 | Acc: 0.1402\n",
      "Step 19/20 | Acc: 0.1293\n",
      "Step 20/20 | Acc: 0.1251\n",
      "Epoch 1/100 | train_loss_predictor=1.5116 | Val Accuracy: 0.1434|0.1434, Val Metric_predictor: 0.5858lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1258\n",
      "Step 2/20 | Acc: 0.1292\n",
      "Step 3/20 | Acc: 0.1228\n",
      "Step 4/20 | Acc: 0.1236\n",
      "Step 5/20 | Acc: 0.1304\n",
      "Step 6/20 | Acc: 0.1342\n",
      "Step 7/20 | Acc: 0.1380\n",
      "Step 8/20 | Acc: 0.1389\n",
      "Step 9/20 | Acc: 0.1380\n",
      "Step 10/20 | Acc: 0.1397\n",
      "Step 11/20 | Acc: 0.1425\n",
      "Step 12/20 | Acc: 0.1402\n",
      "Step 13/20 | Acc: 0.1365\n",
      "Step 14/20 | Acc: 0.1347\n",
      "Step 15/20 | Acc: 0.1335\n",
      "Step 16/20 | Acc: 0.1289\n",
      "Step 17/20 | Acc: 0.1267\n",
      "Step 18/20 | Acc: 0.1262\n",
      "Step 19/20 | Acc: 0.1251\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 2/100 | train_loss_predictor=1.1690 | Val Accuracy: 0.1320|0.1434, Val Metric_predictor: 0.5900lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1242\n",
      "Step 2/20 | Acc: 0.1274\n",
      "Step 3/20 | Acc: 0.1293\n",
      "Step 4/20 | Acc: 0.1255\n",
      "Step 5/20 | Acc: 0.1277\n",
      "Step 6/20 | Acc: 0.1289\n",
      "Step 7/20 | Acc: 0.1346\n",
      "Step 8/20 | Acc: 0.1375\n",
      "Step 9/20 | Acc: 0.1364\n",
      "Step 10/20 | Acc: 0.1342\n",
      "Step 11/20 | Acc: 0.1329\n",
      "Step 12/20 | Acc: 0.1314\n",
      "Step 13/20 | Acc: 0.1304\n",
      "Step 14/20 | Acc: 0.1282\n",
      "Step 15/20 | Acc: 0.1280\n",
      "Step 16/20 | Acc: 0.1258\n",
      "Step 17/20 | Acc: 0.1251\n",
      "Step 18/20 | Acc: 0.1250\n",
      "Step 19/20 | Acc: 0.1250\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 3/100 | train_loss_predictor=1.1079 | Val Accuracy: 0.1291|0.1434, Val Metric_predictor: 0.6029lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1238\n",
      "Step 2/20 | Acc: 0.1234\n",
      "Step 3/20 | Acc: 0.1229\n",
      "Step 4/20 | Acc: 0.1225\n",
      "Step 5/20 | Acc: 0.1243\n",
      "Step 6/20 | Acc: 0.1212\n",
      "Step 7/20 | Acc: 0.1210\n",
      "Step 8/20 | Acc: 0.1226\n",
      "Step 9/20 | Acc: 0.1244\n",
      "Step 10/20 | Acc: 0.1270\n",
      "Step 11/20 | Acc: 0.1269\n",
      "Step 12/20 | Acc: 0.1281\n",
      "Step 13/20 | Acc: 0.1287\n",
      "Step 14/20 | Acc: 0.1288\n",
      "Step 15/20 | Acc: 0.1281\n",
      "Step 16/20 | Acc: 0.1268\n",
      "Step 17/20 | Acc: 0.1266\n",
      "Step 18/20 | Acc: 0.1266\n",
      "Step 19/20 | Acc: 0.1250\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 4/100 | train_loss_predictor=1.0880 | Val Accuracy: 0.1252|0.1434, Val Metric_predictor: 0.6036lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1247\n",
      "Step 2/20 | Acc: 0.1203\n",
      "Step 3/20 | Acc: 0.1176\n",
      "Step 4/20 | Acc: 0.1178\n",
      "Step 5/20 | Acc: 0.1215\n",
      "Step 6/20 | Acc: 0.1188\n",
      "Step 7/20 | Acc: 0.1242\n",
      "Step 8/20 | Acc: 0.1265\n",
      "Step 9/20 | Acc: 0.1252\n",
      "Step 10/20 | Acc: 0.1257\n",
      "Step 11/20 | Acc: 0.1298\n",
      "Step 12/20 | Acc: 0.1308\n",
      "Step 13/20 | Acc: 0.1278\n",
      "Step 14/20 | Acc: 0.1264\n",
      "Step 15/20 | Acc: 0.1259\n",
      "Step 16/20 | Acc: 0.1254\n",
      "Step 17/20 | Acc: 0.1253\n",
      "Step 18/20 | Acc: 0.1252\n",
      "Step 19/20 | Acc: 0.1250\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 5/100 | train_loss_predictor=1.0701 | Val Accuracy: 0.1244|0.1434, Val Metric_predictor: 0.6163lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1219\n",
      "Step 2/20 | Acc: 0.1194\n",
      "Step 3/20 | Acc: 0.1171\n",
      "Step 4/20 | Acc: 0.1218\n",
      "Step 5/20 | Acc: 0.1201\n",
      "Step 6/20 | Acc: 0.1230\n",
      "Step 7/20 | Acc: 0.1247\n",
      "Step 8/20 | Acc: 0.1269\n",
      "Step 9/20 | Acc: 0.1265\n",
      "Step 10/20 | Acc: 0.1264\n",
      "Step 11/20 | Acc: 0.1266\n",
      "Step 12/20 | Acc: 0.1272\n",
      "Step 13/20 | Acc: 0.1266\n",
      "Step 14/20 | Acc: 0.1249\n",
      "Step 15/20 | Acc: 0.1245\n",
      "Step 16/20 | Acc: 0.1252\n",
      "Step 17/20 | Acc: 0.1251\n",
      "Step 18/20 | Acc: 0.1247\n",
      "Step 19/20 | Acc: 0.1250\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 6/100 | train_loss_predictor=1.0599 | Val Accuracy: 0.1241|0.1434, Val Metric_predictor: 0.6033lr_predictor=0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1212\n",
      "Step 2/20 | Acc: 0.1246\n",
      "Step 3/20 | Acc: 0.1223\n",
      "Step 4/20 | Acc: 0.1201\n",
      "Step 5/20 | Acc: 0.1188\n",
      "Step 6/20 | Acc: 0.1225\n",
      "Step 7/20 | Acc: 0.1206\n",
      "Step 8/20 | Acc: 0.1191\n",
      "Step 9/20 | Acc: 0.1195\n",
      "Step 10/20 | Acc: 0.1212\n",
      "Step 11/20 | Acc: 0.1233\n",
      "Step 12/20 | Acc: 0.1234\n",
      "Step 13/20 | Acc: 0.1251\n",
      "Step 14/20 | Acc: 0.1256\n",
      "Step 15/20 | Acc: 0.1277\n",
      "Step 16/20 | Acc: 0.1273\n",
      "Step 17/20 | Acc: 0.1259\n",
      "Step 18/20 | Acc: 0.1249\n",
      "Step 19/20 | Acc: 0.1250\n",
      "Step 20/20 | Acc: 0.1250\n",
      "Epoch 7/100 | train_loss_predictor=1.0503 | Val Accuracy: 0.1232|0.1434, Val Metric_predictor: 0.5980lr_predictor=0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413029/773552123.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n",
      "/tmp/ipykernel_413029/773552123.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mb = torch.tensor(m_np, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/20 (no features) | Acc: 0.1250\n",
      "Step 1/20 | Acc: 0.1268\n",
      "Step 2/20 | Acc: 0.1258\n",
      "Step 3/20 | Acc: 0.1197\n",
      "Step 4/20 | Acc: 0.1181\n",
      "Step 5/20 | Acc: 0.1192\n",
      "Step 6/20 | Acc: 0.1212\n",
      "Step 7/20 | Acc: 0.1222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m TensorDataset(X_val, y_val)\n\u001b[1;32m     35\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_val), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcooldown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m, in \u001b[0;36mtrain_predictor\u001b[0;34m(predictor, generator, train_loader, val_loader, epochs, optimizer_predictor, criterion, metric_f, save_path, lr_factor, cooldown, min_lr, scheduler_patience)\u001b[0m\n\u001b[1;32m     76\u001b[0m     total_val_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs_val\n\u001b[1;32m     78\u001b[0m val_metric_predictor \u001b[38;5;241m=\u001b[39m total_metric_predictor \u001b[38;5;241m/\u001b[39m total_val_samples\n\u001b[0;32m---> 79\u001b[0m val_auc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_feature_acquisition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m scheduler_predictor\u001b[38;5;241m.\u001b[39mstep(val_auc)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_auc \u001b[38;5;241m==\u001b[39m scheduler_predictor\u001b[38;5;241m.\u001b[39mbest:\n",
      "Cell \u001b[0;32mIn[18], line 37\u001b[0m, in \u001b[0;36mrun_feature_acquisition\u001b[0;34m(predictor, generator, val_loader, metric_f, num_samples, alpha, gamma)\u001b[0m\n\u001b[1;32m     26\u001b[0m FA \u001b[38;5;241m=\u001b[39m FeatureAcquisition(\n\u001b[1;32m     27\u001b[0m     x\u001b[38;5;241m=\u001b[39mxb,\n\u001b[1;32m     28\u001b[0m     m\u001b[38;5;241m=\u001b[39mmv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mgamma\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# mask 업데이트\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m mv, _ \u001b[38;5;241m=\u001b[39m \u001b[43mFA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m step_acc \u001b[38;5;241m=\u001b[39m metric_f(predictor(xb, mv), yb)\n\u001b[1;32m     39\u001b[0m val_metrics\u001b[38;5;241m.\u001b[39mappend(step_acc)\n",
      "File \u001b[0;32m~/gamm2/Gamma-CMI/FeatureAcquisition.py:84\u001b[0m, in \u001b[0;36mFeatureAcquisition.acquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm\n\u001b[1;32m     83\u001b[0m m_np \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 84\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_gamma_cmi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m scores \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m     86\u001b[0m scores \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m m_np)\n",
      "File \u001b[0;32m~/gamm2/Gamma-CMI/FeatureAcquisition.py:51\u001b[0m, in \u001b[0;36mFeatureAcquisition.alpha_gamma_cmi\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\n\u001b[1;32m     49\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(predictor\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 51\u001b[0m x_sampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 전체 값이 샘플링된 x\u001b[39;00m\n\u001b[1;32m     52\u001b[0m m_repeated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(a\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), repeats\u001b[38;5;241m=\u001b[39mnum_samples, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# 기존 x의 mask\u001b[39;00m\n\u001b[1;32m     54\u001b[0m m_upsampled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39mgamma, size\u001b[38;5;241m=\u001b[39mm_repeated\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# 각 feature별로 0 또는 1로 변형 \u001b[39;00m\n",
      "File \u001b[0;32m~/gamm2/Gamma-CMI/FeatureAcquisition.py:38\u001b[0m, in \u001b[0;36mFeatureAcquisition.conditional_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m x_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(a\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), repeats\u001b[38;5;241m=\u001b[39mnum_samples, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m m_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(a\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), repeats\u001b[38;5;241m=\u001b[39mnum_samples, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m x_sampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerative_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_sampled\n",
      "File \u001b[0;32m~/miniconda3/envs/afa/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gamm2/Gamma-CMI/PartialVAE.py:208\u001b[0m, in \u001b[0;36mPartialVAE.generate\u001b[0;34m(self, x, m, stochastic, obs_sigma)\u001b[0m\n\u001b[1;32m    204\u001b[0m x_hat, (mu, sig) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xt, mt, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    206\u001b[0m x_fill \u001b[38;5;241m=\u001b[39m mt \u001b[38;5;241m*\u001b[39m xt \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mt) \u001b[38;5;241m*\u001b[39m x_hat\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx_fill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "dataset_name = dataset_dict[\"dataset\"]\n",
    "num_con_features = dataset_dict[\"num_con_features\"]\n",
    "num_cat_features = dataset_dict[\"num_cat_features\"]\n",
    "num_classes = dataset_dict[\"out_dim\"]\n",
    "metric_f = metrics_dict[dataset_dict[\"metric\"]]\n",
    "save_path = os.path.join(ROOT_DIR, \"saved_models\", dataset_name)\n",
    "\n",
    "\n",
    "predictor = Predictor(feature_dim=num_con_features + num_cat_features, num_classes=num_classes, hidden_dim=128)\n",
    "optimizer_predictor = AdamW(predictor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "pvae = PartialVAE(\n",
    "    input_type=\"continuous\",\n",
    "    num_con_features=num_con_features,\n",
    "    num_cat_features=num_cat_features,\n",
    "    hidden_dim_con=30,\n",
    "    most_categories=max(1, 0),   # 내부 차원 계산을 위해 최소 1\n",
    "    c_dim=16,\n",
    "    hid_enc=100,\n",
    "    hid_dec=100,\n",
    "    latent_dim=30\n",
    ")\n",
    "# 훈련한 generator 불러오기\n",
    "pvae.load_state_dict(torch.load(os.path.join(save_path, \"best_model_generator.pt\")))\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_ds, batch_size=len(X_val), shuffle=False, drop_last=False)\n",
    "\n",
    "train_predictor(\n",
    "    predictor=predictor,\n",
    "    generator=pvae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs,\n",
    "    optimizer_predictor=optimizer_predictor,\n",
    "    criterion=criterion,\n",
    "    metric_f=metric_f,\n",
    "    save_path=save_path,\n",
    "    lr_factor=0.2,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,\n",
    "    scheduler_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/20 | Acc: 0.3142\n",
      "Step 2/20 | Acc: 0.5485\n",
      "Step 3/20 | Acc: 0.7354\n",
      "Step 4/20 | Acc: 0.8492\n",
      "Step 5/20 | Acc: 0.9076\n",
      "Step 6/20 | Acc: 0.9382\n",
      "Step 7/20 | Acc: 0.9542\n",
      "Step 8/20 | Acc: 0.9593\n",
      "Step 9/20 | Acc: 0.9614\n",
      "Step 10/20 | Acc: 0.9619\n",
      "Step 11/20 | Acc: 0.9621\n",
      "Step 12/20 | Acc: 0.9620\n",
      "Step 13/20 | Acc: 0.9621\n",
      "Step 14/20 | Acc: 0.9618\n",
      "Step 15/20 | Acc: 0.9618\n",
      "Step 16/20 | Acc: 0.9615\n",
      "Step 17/20 | Acc: 0.9612\n",
      "Step 18/20 | Acc: 0.9608\n",
      "Step 19/20 | Acc: 0.9601\n",
      "Step 20/20 | Acc: 0.9597\n"
     ]
    }
   ],
   "source": [
    "predictor.eval()\n",
    "N = X_test.size(0)\n",
    "D = X_test.size(1)\n",
    "x_np = X_test.cpu().numpy()\n",
    "m_np = np.zeros((N, D), dtype=np.float32)\n",
    "\n",
    "result = []\n",
    "\n",
    "accs = run_feature_acquisition(\n",
    "    predictor=predictor,\n",
    "    generator=pvae,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    x_np=x_np,\n",
    "    m_np=m_np,\n",
    "    D=D,\n",
    "    num_samples=10,\n",
    "    alpha=1,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "result.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "전체 평균: 0.88714993\n",
      "표준편차: 0.0\n",
      "acquisition 별 평균): [0.3142 0.5485 0.7354 0.8492 0.9076 0.9382 0.9542 0.9593 0.9614 0.9619\n",
      " 0.9621 0.962  0.9621 0.9618 0.9618 0.9615 0.9612 0.9608 0.9601 0.9597]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_get_renderer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/matplotlib/backend_bases.py:2353\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         bbox_inches \u001b[38;5;241m=\u001b[39m bbox_inches\u001b[38;5;241m.\u001b[39mpadded(pad_inches)\n\u001b[1;32m   2352\u001b[0m     \u001b[38;5;66;03m# call adjust_bbox to save only the given area\u001b[39;00m\n\u001b[0;32m-> 2353\u001b[0m     restore_bbox \u001b[38;5;241m=\u001b[39m \u001b[43m_tight_bbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_bbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_inches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_dpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2356\u001b[0m     _bbox_inches_restore \u001b[38;5;241m=\u001b[39m (bbox_inches, restore_bbox)\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/matplotlib/_tight_bbox.py:28\u001b[0m, in \u001b[0;36madjust_bbox\u001b[0;34m(fig, bbox_inches, fixed_dpi)\u001b[0m\n\u001b[1;32m     26\u001b[0m locator \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_axes_locator()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m locator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     ax\u001b[38;5;241m.\u001b[39mapply_aspect(\u001b[43mlocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m locator_list\u001b[38;5;241m.\u001b[39mappend(locator)\n\u001b[1;32m     30\u001b[0m current_pos \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_position(original\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfrozen()\n",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/mpl_toolkits/axes_grid1/inset_locator.py:73\u001b[0m, in \u001b[0;36mAnchoredLocatorBase.__call__\u001b[0;34m(self, ax, renderer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ax, renderer):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m ax\n\u001b[0;32m---> 73\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     px, py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_offset(bbox\u001b[38;5;241m.\u001b[39mwidth, bbox\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, renderer)\n\u001b[1;32m     75\u001b[0m     bbox_canvas \u001b[38;5;241m=\u001b[39m Bbox\u001b[38;5;241m.\u001b[39mfrom_bounds(px, py, bbox\u001b[38;5;241m.\u001b[39mwidth, bbox\u001b[38;5;241m.\u001b[39mheight)\n",
      "File \u001b[0;32m~/miniconda3/envs/sefa_env2/lib/python3.8/site-packages/matplotlib/offsetbox.py:399\u001b[0m, in \u001b[0;36mOffsetBox.get_window_extent\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_window_extent\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m         renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_renderer\u001b[49m()\n\u001b[1;32m    400\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_bbox(renderer)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# Some subclasses redefine get_offset to take no args.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_get_renderer'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ACC over all acquisitions = 0.8871\n"
     ]
    }
   ],
   "source": [
    "result = np.array(result, dtype=np.float32)\n",
    "\n",
    "num_runs, D = result.shape\n",
    "print(result.shape)\n",
    "row_means = result.mean(axis=1)\n",
    "overall_mean = row_means.mean()\n",
    "\n",
    "std = row_means.std()\n",
    "\n",
    "col_means = result.mean(axis=0)\n",
    "\n",
    "print(\"전체 평균:\", overall_mean)\n",
    "print(\"표준편차:\", std)\n",
    "print(\"acquisition 별 평균):\", col_means)\n",
    "\n",
    "# 시각화\n",
    "xs = np.arange(1, D + 1)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(xs, col_means, marker='o')\n",
    "plt.xlabel('Acquisition No.', fontsize=11)\n",
    "plt.ylabel('ACCURACY', fontsize=11)\n",
    "plt.title('Cube', fontsize=12)\n",
    "plt.xticks(np.arange(0, D+1, 2))\n",
    "plt.xlim(0, D + 1)\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 확대 영역\n",
    "ax = plt.gca()\n",
    "axins = inset_axes(ax, width=\"50%\", height=\"50%\", loc='center right')\n",
    "\n",
    "axins.plot(xs, col_means, marker='o')\n",
    "axins.set_xlim(6, 8)\n",
    "axins.set_ylim(0.92, 0.98)\n",
    "axins.grid(True, alpha=0.3)\n",
    "axins.tick_params(labelsize=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "mean_acc = np.nanmean(col_means)\n",
    "print(f\"Mean ACC over all acquisitions = {mean_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91eed8",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f84bf0",
   "metadata": {},
   "source": [
    "# Bank Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ada1a",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2995f5",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc22ac",
   "metadata": {},
   "source": [
    "# California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\", \"california_housing\")\n",
    "\n",
    "X_train = torch.load(f\"{DATA_DIR}/X_train_cdf.pt\").float()\n",
    "y_train = torch.load(f\"{DATA_DIR}/y_train.pt\").long()\n",
    "\n",
    "X_val   = torch.load(f\"{DATA_DIR}/X_val_cdf.pt\").float()\n",
    "y_val   = torch.load(f\"{DATA_DIR}/y_val.pt\").long()\n",
    "\n",
    "X_test = torch.load(f\"{DATA_DIR}/X_test_cdf.pt\").float()\n",
    "y_test = torch.load(f\"{DATA_DIR}/y_test.pt\").long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144270fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | train_loss=1.3093 | val_loss=1.2060 | val_acc=0.4370 | lr=0.001000\n",
      "Epoch 2/100 | train_loss=1.2101 | val_loss=1.1537 | val_acc=0.4714 | lr=0.001000\n",
      "Epoch 3/100 | train_loss=1.1858 | val_loss=1.1568 | val_acc=0.4680 | lr=0.001000\n",
      "Epoch 4/100 | train_loss=1.1785 | val_loss=1.1428 | val_acc=0.4612 | lr=0.001000\n",
      "Epoch 5/100 | train_loss=1.1597 | val_loss=1.1220 | val_acc=0.4767 | lr=0.001000\n",
      "Epoch 6/100 | train_loss=1.1466 | val_loss=1.1217 | val_acc=0.5029 | lr=0.001000\n",
      "Epoch 7/100 | train_loss=1.1418 | val_loss=1.1207 | val_acc=0.4913 | lr=0.001000\n",
      "Epoch 8/100 | train_loss=1.1353 | val_loss=1.1169 | val_acc=0.5015 | lr=0.001000\n",
      "Epoch 9/100 | train_loss=1.1353 | val_loss=1.1090 | val_acc=0.4864 | lr=0.001000\n",
      "Epoch 10/100 | train_loss=1.1294 | val_loss=1.1201 | val_acc=0.4748 | lr=0.001000\n",
      "Epoch 11/100 | train_loss=1.1344 | val_loss=1.1062 | val_acc=0.4927 | lr=0.001000\n",
      "Epoch 12/100 | train_loss=1.1199 | val_loss=1.1038 | val_acc=0.5000 | lr=0.000200\n",
      "Epoch 13/100 | train_loss=1.1256 | val_loss=1.1025 | val_acc=0.4811 | lr=0.000200\n",
      "Epoch 14/100 | train_loss=1.1235 | val_loss=1.1145 | val_acc=0.4792 | lr=0.000200\n",
      "Epoch 15/100 | train_loss=1.1218 | val_loss=1.0983 | val_acc=0.4952 | lr=0.000200\n",
      "Epoch 16/100 | train_loss=1.1247 | val_loss=1.0902 | val_acc=0.5000 | lr=0.000200\n",
      "Epoch 17/100 | train_loss=1.1281 | val_loss=1.0970 | val_acc=0.4903 | lr=0.000200\n",
      "Epoch 18/100 | train_loss=1.1233 | val_loss=1.1041 | val_acc=0.4937 | lr=0.000040\n",
      "Epoch 19/100 | train_loss=1.1215 | val_loss=1.0947 | val_acc=0.4927 | lr=0.000040\n",
      "Epoch 20/100 | train_loss=1.1252 | val_loss=1.0835 | val_acc=0.4947 | lr=0.000040\n",
      "Epoch 21/100 | train_loss=1.1218 | val_loss=1.1167 | val_acc=0.4864 | lr=0.000040\n",
      "Epoch 22/100 | train_loss=1.1242 | val_loss=1.1078 | val_acc=0.4927 | lr=0.000040\n",
      "Epoch 23/100 | train_loss=1.1209 | val_loss=1.1100 | val_acc=0.4995 | lr=0.000040\n",
      "Epoch 24/100 | train_loss=1.1252 | val_loss=1.0941 | val_acc=0.4855 | lr=0.000008\n",
      "Epoch 25/100 | train_loss=1.1181 | val_loss=1.1017 | val_acc=0.4898 | lr=0.000008\n",
      "Epoch 26/100 | train_loss=1.1160 | val_loss=1.0804 | val_acc=0.4976 | lr=0.000008\n",
      "Epoch 27/100 | train_loss=1.1185 | val_loss=1.1024 | val_acc=0.4995 | lr=0.000008\n",
      "Epoch 28/100 | train_loss=1.1156 | val_loss=1.0882 | val_acc=0.4913 | lr=0.000008\n",
      "Epoch 29/100 | train_loss=1.1203 | val_loss=1.1011 | val_acc=0.4922 | lr=0.000008\n",
      "Epoch 30/100 | train_loss=1.1229 | val_loss=1.1137 | val_acc=0.4797 | lr=0.000002\n",
      "Epoch 31/100 | train_loss=1.1191 | val_loss=1.1022 | val_acc=0.5024 | lr=0.000002\n",
      "Epoch 32/100 | train_loss=1.1190 | val_loss=1.1077 | val_acc=0.4898 | lr=0.000002\n",
      "Epoch 33/100 | train_loss=1.1240 | val_loss=1.1021 | val_acc=0.4922 | lr=0.000002\n",
      "Epoch 34/100 | train_loss=1.1261 | val_loss=1.1053 | val_acc=0.4782 | lr=0.000002\n",
      "Epoch 35/100 | train_loss=1.1221 | val_loss=1.1082 | val_acc=0.4874 | lr=0.000002\n",
      "Epoch 36/100 | train_loss=1.1210 | val_loss=1.1005 | val_acc=0.4956 | lr=0.000000\n",
      "Epoch 37/100 | train_loss=1.1183 | val_loss=1.0941 | val_acc=0.4981 | lr=0.000000\n",
      "Epoch 38/100 | train_loss=1.1210 | val_loss=1.0982 | val_acc=0.4898 | lr=0.000000\n",
      "Epoch 39/100 | train_loss=1.1258 | val_loss=1.0987 | val_acc=0.4884 | lr=0.000000\n",
      "Epoch 40/100 | train_loss=1.1222 | val_loss=1.1037 | val_acc=0.4922 | lr=0.000000\n",
      "Epoch 41/100 | train_loss=1.1159 | val_loss=1.0971 | val_acc=0.5005 | lr=0.000000\n",
      "Epoch 42/100 | train_loss=1.1171 | val_loss=1.0919 | val_acc=0.4971 | lr=0.000000\n",
      "Epoch 43/100 | train_loss=1.1179 | val_loss=1.1373 | val_acc=0.4695 | lr=0.000000\n",
      "Epoch 44/100 | train_loss=1.1162 | val_loss=1.0910 | val_acc=0.4932 | lr=0.000000\n",
      "Epoch 45/100 | train_loss=1.1179 | val_loss=1.0980 | val_acc=0.4981 | lr=0.000000\n",
      "Epoch 46/100 | train_loss=1.1225 | val_loss=1.0916 | val_acc=0.5082 | lr=0.000000\n",
      "Epoch 47/100 | train_loss=1.1237 | val_loss=1.1066 | val_acc=0.4932 | lr=0.000000\n",
      "Epoch 48/100 | train_loss=1.1107 | val_loss=1.0974 | val_acc=0.4918 | lr=0.000000\n",
      "Epoch 49/100 | train_loss=1.1217 | val_loss=1.0887 | val_acc=0.5039 | lr=0.000000\n",
      "Epoch 50/100 | train_loss=1.1198 | val_loss=1.1112 | val_acc=0.4801 | lr=0.000000\n",
      "Epoch 51/100 | train_loss=1.1192 | val_loss=1.1082 | val_acc=0.4918 | lr=0.000000\n",
      "Epoch 52/100 | train_loss=1.1186 | val_loss=1.1087 | val_acc=0.4913 | lr=0.000000\n",
      "Epoch 53/100 | train_loss=1.1149 | val_loss=1.0910 | val_acc=0.4922 | lr=0.000000\n",
      "Epoch 54/100 | train_loss=1.1193 | val_loss=1.0956 | val_acc=0.5044 | lr=0.000000\n",
      "Epoch 55/100 | train_loss=1.1200 | val_loss=1.1122 | val_acc=0.4908 | lr=0.000000\n",
      "Epoch 56/100 | train_loss=1.1190 | val_loss=1.1132 | val_acc=0.4971 | lr=0.000000\n",
      "Epoch 57/100 | train_loss=1.1175 | val_loss=1.1139 | val_acc=0.4903 | lr=0.000000\n",
      "Epoch 58/100 | train_loss=1.1181 | val_loss=1.1013 | val_acc=0.4884 | lr=0.000000\n",
      "Epoch 59/100 | train_loss=1.1165 | val_loss=1.0946 | val_acc=0.5044 | lr=0.000000\n",
      "Epoch 60/100 | train_loss=1.1289 | val_loss=1.0958 | val_acc=0.4884 | lr=0.000000\n",
      "Epoch 61/100 | train_loss=1.1152 | val_loss=1.1129 | val_acc=0.4806 | lr=0.000000\n",
      "Epoch 62/100 | train_loss=1.1198 | val_loss=1.1191 | val_acc=0.4743 | lr=0.000000\n",
      "Epoch 63/100 | train_loss=1.1252 | val_loss=1.0996 | val_acc=0.4903 | lr=0.000000\n",
      "Epoch 64/100 | train_loss=1.1214 | val_loss=1.0986 | val_acc=0.5010 | lr=0.000000\n",
      "Epoch 65/100 | train_loss=1.1146 | val_loss=1.0808 | val_acc=0.5131 | lr=0.000000\n",
      "Epoch 66/100 | train_loss=1.1175 | val_loss=1.0998 | val_acc=0.4898 | lr=0.000000\n",
      "Epoch 67/100 | train_loss=1.1200 | val_loss=1.0972 | val_acc=0.4956 | lr=0.000000\n",
      "Epoch 68/100 | train_loss=1.1223 | val_loss=1.1077 | val_acc=0.4806 | lr=0.000000\n",
      "Epoch 69/100 | train_loss=1.1187 | val_loss=1.0867 | val_acc=0.5010 | lr=0.000000\n",
      "Epoch 70/100 | train_loss=1.1154 | val_loss=1.1114 | val_acc=0.4874 | lr=0.000000\n",
      "Epoch 71/100 | train_loss=1.1153 | val_loss=1.1096 | val_acc=0.4855 | lr=0.000000\n",
      "Epoch 72/100 | train_loss=1.1221 | val_loss=1.0986 | val_acc=0.4942 | lr=0.000000\n",
      "Epoch 73/100 | train_loss=1.1162 | val_loss=1.1090 | val_acc=0.4908 | lr=0.000000\n",
      "Epoch 74/100 | train_loss=1.1217 | val_loss=1.1016 | val_acc=0.5010 | lr=0.000000\n",
      "Epoch 75/100 | train_loss=1.1198 | val_loss=1.0816 | val_acc=0.5068 | lr=0.000000\n",
      "Epoch 76/100 | train_loss=1.1152 | val_loss=1.1113 | val_acc=0.4922 | lr=0.000000\n",
      "Epoch 77/100 | train_loss=1.1172 | val_loss=1.1041 | val_acc=0.4956 | lr=0.000000\n",
      "Epoch 78/100 | train_loss=1.1138 | val_loss=1.0947 | val_acc=0.4956 | lr=0.000000\n",
      "Epoch 79/100 | train_loss=1.1205 | val_loss=1.0838 | val_acc=0.5029 | lr=0.000000\n",
      "Epoch 80/100 | train_loss=1.1194 | val_loss=1.1050 | val_acc=0.4937 | lr=0.000000\n",
      "Epoch 81/100 | train_loss=1.1156 | val_loss=1.0945 | val_acc=0.5044 | lr=0.000000\n",
      "Epoch 82/100 | train_loss=1.1226 | val_loss=1.1083 | val_acc=0.4850 | lr=0.000000\n",
      "Epoch 83/100 | train_loss=1.1188 | val_loss=1.0903 | val_acc=0.4947 | lr=0.000000\n",
      "Epoch 84/100 | train_loss=1.1228 | val_loss=1.0894 | val_acc=0.5102 | lr=0.000000\n",
      "Epoch 85/100 | train_loss=1.1182 | val_loss=1.0981 | val_acc=0.4830 | lr=0.000000\n",
      "Epoch 86/100 | train_loss=1.1125 | val_loss=1.0914 | val_acc=0.4884 | lr=0.000000\n",
      "Epoch 87/100 | train_loss=1.1186 | val_loss=1.0953 | val_acc=0.4927 | lr=0.000000\n",
      "Epoch 88/100 | train_loss=1.1160 | val_loss=1.1014 | val_acc=0.4859 | lr=0.000000\n",
      "Epoch 89/100 | train_loss=1.1166 | val_loss=1.1036 | val_acc=0.4913 | lr=0.000000\n",
      "Epoch 90/100 | train_loss=1.1226 | val_loss=1.1050 | val_acc=0.4884 | lr=0.000000\n",
      "Epoch 91/100 | train_loss=1.1207 | val_loss=1.0920 | val_acc=0.4932 | lr=0.000000\n",
      "Epoch 92/100 | train_loss=1.1208 | val_loss=1.0934 | val_acc=0.4874 | lr=0.000000\n",
      "Epoch 93/100 | train_loss=1.1161 | val_loss=1.1021 | val_acc=0.4879 | lr=0.000000\n",
      "Epoch 94/100 | train_loss=1.1155 | val_loss=1.1103 | val_acc=0.4922 | lr=0.000000\n",
      "Epoch 95/100 | train_loss=1.1139 | val_loss=1.0841 | val_acc=0.4981 | lr=0.000000\n",
      "Epoch 96/100 | train_loss=1.1250 | val_loss=1.1004 | val_acc=0.4942 | lr=0.000000\n",
      "Epoch 97/100 | train_loss=1.1210 | val_loss=1.1038 | val_acc=0.4956 | lr=0.000000\n",
      "Epoch 98/100 | train_loss=1.1240 | val_loss=1.1148 | val_acc=0.4758 | lr=0.000000\n",
      "Epoch 99/100 | train_loss=1.1193 | val_loss=1.0960 | val_acc=0.4903 | lr=0.000000\n",
      "Epoch 100/100 | train_loss=1.1185 | val_loss=1.0998 | val_acc=0.4942 | lr=0.000000\n",
      "Best validation accuracy = 0.5131\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "D = 8  # feature 개수 고정\n",
    "\n",
    "predictor = Predictor(feature_dim=8, num_classes=4, hidden_dim=128)\n",
    "optimizer = AdamW(predictor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "train_predictor(\n",
    "    predictor=predictor,\n",
    "    train_loader=train_loader,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    D=D,\n",
    "    epochs=epochs,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PVAE ep 00] train_loss=11.7871  train_KL=1.8051  train_NLL_X=9.9820 | val_loss=9.3578  val_KL=1.9682  val_NLL_X=7.3896 | lr=0.001000\n",
      "[PVAE ep 01] train_loss=8.4565  train_KL=2.1349  train_NLL_X=6.3216 | val_loss=8.0482  val_KL=2.3415  val_NLL_X=5.7067 | lr=0.001000\n",
      "[PVAE ep 02] train_loss=7.6641  train_KL=2.3469  train_NLL_X=5.3172 | val_loss=7.6073  val_KL=2.5025  val_NLL_X=5.1048 | lr=0.001000\n",
      "[PVAE ep 03] train_loss=7.2944  train_KL=2.4704  train_NLL_X=4.8240 | val_loss=7.2036  val_KL=2.6113  val_NLL_X=4.5923 | lr=0.001000\n",
      "[PVAE ep 04] train_loss=7.0381  train_KL=2.6366  train_NLL_X=4.4015 | val_loss=7.0186  val_KL=2.8248  val_NLL_X=4.1938 | lr=0.001000\n",
      "[PVAE ep 05] train_loss=6.7740  train_KL=2.6870  train_NLL_X=4.0870 | val_loss=6.7994  val_KL=2.7823  val_NLL_X=4.0171 | lr=0.001000\n",
      "[PVAE ep 06] train_loss=6.6118  train_KL=2.7530  train_NLL_X=3.8587 | val_loss=6.4991  val_KL=2.7722  val_NLL_X=3.7270 | lr=0.001000\n",
      "[PVAE ep 07] train_loss=6.5102  train_KL=2.7808  train_NLL_X=3.7293 | val_loss=6.5825  val_KL=2.9918  val_NLL_X=3.5906 | lr=0.001000\n",
      "[PVAE ep 08] train_loss=6.3422  train_KL=2.8308  train_NLL_X=3.5114 | val_loss=6.2906  val_KL=2.9102  val_NLL_X=3.3805 | lr=0.001000\n",
      "[PVAE ep 09] train_loss=6.2453  train_KL=2.8408  train_NLL_X=3.4045 | val_loss=6.2183  val_KL=2.8762  val_NLL_X=3.3422 | lr=0.001000\n",
      "[PVAE ep 10] train_loss=6.1910  train_KL=2.8671  train_NLL_X=3.3240 | val_loss=6.1881  val_KL=2.8597  val_NLL_X=3.3284 | lr=0.001000\n",
      "[PVAE ep 11] train_loss=6.1466  train_KL=2.8479  train_NLL_X=3.2987 | val_loss=6.1715  val_KL=2.8677  val_NLL_X=3.3039 | lr=0.001000\n",
      "[PVAE ep 12] train_loss=6.1370  train_KL=2.8822  train_NLL_X=3.2548 | val_loss=6.2400  val_KL=2.8206  val_NLL_X=3.4194 | lr=0.001000\n",
      "[PVAE ep 13] train_loss=6.0801  train_KL=2.8681  train_NLL_X=3.2120 | val_loss=6.1571  val_KL=2.9142  val_NLL_X=3.2429 | lr=0.001000\n",
      "[PVAE ep 14] train_loss=6.0450  train_KL=2.8435  train_NLL_X=3.2016 | val_loss=6.0774  val_KL=2.9426  val_NLL_X=3.1348 | lr=0.001000\n",
      "[PVAE ep 15] train_loss=6.0344  train_KL=2.8873  train_NLL_X=3.1472 | val_loss=6.0474  val_KL=2.8980  val_NLL_X=3.1494 | lr=0.001000\n",
      "[PVAE ep 16] train_loss=6.0730  train_KL=2.9188  train_NLL_X=3.1543 | val_loss=6.0337  val_KL=2.9554  val_NLL_X=3.0782 | lr=0.001000\n",
      "[PVAE ep 17] train_loss=5.9866  train_KL=2.8799  train_NLL_X=3.1067 | val_loss=5.9877  val_KL=2.8186  val_NLL_X=3.1691 | lr=0.001000\n",
      "[PVAE ep 18] train_loss=6.0206  train_KL=2.9187  train_NLL_X=3.1018 | val_loss=6.0308  val_KL=2.9279  val_NLL_X=3.1029 | lr=0.001000\n",
      "[PVAE ep 19] train_loss=6.0111  train_KL=2.9419  train_NLL_X=3.0691 | val_loss=5.9200  val_KL=2.9765  val_NLL_X=2.9434 | lr=0.001000\n",
      "[PVAE ep 20] train_loss=5.9569  train_KL=2.9119  train_NLL_X=3.0450 | val_loss=5.9752  val_KL=2.9100  val_NLL_X=3.0652 | lr=0.001000\n",
      "[PVAE ep 21] train_loss=5.9605  train_KL=2.9385  train_NLL_X=3.0221 | val_loss=6.0069  val_KL=2.9363  val_NLL_X=3.0706 | lr=0.001000\n",
      "[PVAE ep 22] train_loss=5.9401  train_KL=2.9072  train_NLL_X=3.0329 | val_loss=5.9460  val_KL=3.0194  val_NLL_X=2.9266 | lr=0.001000\n",
      "[PVAE ep 23] train_loss=5.9547  train_KL=2.9553  train_NLL_X=2.9994 | val_loss=5.9066  val_KL=2.8991  val_NLL_X=3.0075 | lr=0.001000\n",
      "[PVAE ep 24] train_loss=5.8932  train_KL=2.8898  train_NLL_X=3.0034 | val_loss=5.9468  val_KL=3.0239  val_NLL_X=2.9229 | lr=0.001000\n",
      "[PVAE ep 25] train_loss=5.9359  train_KL=2.9414  train_NLL_X=2.9945 | val_loss=5.9538  val_KL=2.8861  val_NLL_X=3.0678 | lr=0.001000\n",
      "[PVAE ep 26] train_loss=5.8724  train_KL=2.9026  train_NLL_X=2.9697 | val_loss=5.9633  val_KL=2.9615  val_NLL_X=3.0018 | lr=0.001000\n",
      "[PVAE ep 27] train_loss=5.8700  train_KL=2.9171  train_NLL_X=2.9529 | val_loss=5.9058  val_KL=3.0304  val_NLL_X=2.8754 | lr=0.001000\n",
      "[PVAE ep 28] train_loss=5.8979  train_KL=2.9425  train_NLL_X=2.9554 | val_loss=5.8384  val_KL=2.9563  val_NLL_X=2.8821 | lr=0.001000\n",
      "[PVAE ep 29] train_loss=5.8914  train_KL=2.9414  train_NLL_X=2.9500 | val_loss=5.7723  val_KL=2.8690  val_NLL_X=2.9034 | lr=0.001000\n",
      "[PVAE ep 30] train_loss=5.8629  train_KL=2.9123  train_NLL_X=2.9506 | val_loss=5.8587  val_KL=2.9476  val_NLL_X=2.9111 | lr=0.001000\n",
      "[PVAE ep 31] train_loss=5.8554  train_KL=2.8916  train_NLL_X=2.9638 | val_loss=5.8697  val_KL=3.0309  val_NLL_X=2.8388 | lr=0.001000\n",
      "[PVAE ep 32] train_loss=5.8703  train_KL=2.9258  train_NLL_X=2.9445 | val_loss=5.8106  val_KL=2.9278  val_NLL_X=2.8827 | lr=0.001000\n",
      "[PVAE ep 33] train_loss=5.8699  train_KL=2.9336  train_NLL_X=2.9363 | val_loss=5.8977  val_KL=3.0063  val_NLL_X=2.8914 | lr=0.001000\n",
      "[PVAE ep 34] train_loss=5.8441  train_KL=2.9094  train_NLL_X=2.9346 | val_loss=5.8102  val_KL=2.9012  val_NLL_X=2.9091 | lr=0.001000\n",
      "[PVAE ep 35] train_loss=5.8356  train_KL=2.9097  train_NLL_X=2.9259 | val_loss=5.8767  val_KL=2.9886  val_NLL_X=2.8882 | lr=0.000200\n",
      "[PVAE ep 36] train_loss=5.8067  train_KL=2.9316  train_NLL_X=2.8751 | val_loss=5.8042  val_KL=2.9954  val_NLL_X=2.8088 | lr=0.000200\n",
      "[PVAE ep 37] train_loss=5.8018  train_KL=2.9473  train_NLL_X=2.8545 | val_loss=5.8508  val_KL=2.9814  val_NLL_X=2.8694 | lr=0.000200\n",
      "[PVAE ep 38] train_loss=5.8361  train_KL=2.9458  train_NLL_X=2.8904 | val_loss=5.7462  val_KL=2.8980  val_NLL_X=2.8482 | lr=0.000200\n",
      "[PVAE ep 39] train_loss=5.7724  train_KL=2.9052  train_NLL_X=2.8672 | val_loss=5.7516  val_KL=2.9621  val_NLL_X=2.7895 | lr=0.000200\n",
      "[PVAE ep 40] train_loss=5.7778  train_KL=2.9219  train_NLL_X=2.8559 | val_loss=5.7178  val_KL=2.9380  val_NLL_X=2.7798 | lr=0.000200\n",
      "[PVAE ep 41] train_loss=5.7825  train_KL=2.9409  train_NLL_X=2.8416 | val_loss=5.8079  val_KL=2.9546  val_NLL_X=2.8533 | lr=0.000200\n",
      "[PVAE ep 42] train_loss=5.7365  train_KL=2.8961  train_NLL_X=2.8404 | val_loss=5.8053  val_KL=2.9577  val_NLL_X=2.8476 | lr=0.000200\n",
      "[PVAE ep 43] train_loss=5.7609  train_KL=2.8982  train_NLL_X=2.8628 | val_loss=5.7566  val_KL=2.9168  val_NLL_X=2.8397 | lr=0.000200\n",
      "[PVAE ep 44] train_loss=5.7751  train_KL=2.9267  train_NLL_X=2.8484 | val_loss=5.7626  val_KL=2.9553  val_NLL_X=2.8073 | lr=0.000200\n",
      "[PVAE ep 45] train_loss=5.7740  train_KL=2.9399  train_NLL_X=2.8342 | val_loss=5.7327  val_KL=2.9405  val_NLL_X=2.7922 | lr=0.000200\n",
      "[PVAE ep 46] train_loss=5.7857  train_KL=2.9426  train_NLL_X=2.8431 | val_loss=5.8370  val_KL=2.9001  val_NLL_X=2.9369 | lr=0.000040\n",
      "[PVAE ep 47] train_loss=5.7859  train_KL=2.9296  train_NLL_X=2.8564 | val_loss=5.8333  val_KL=2.9745  val_NLL_X=2.8588 | lr=0.000040\n",
      "[PVAE ep 48] train_loss=5.7194  train_KL=2.8959  train_NLL_X=2.8235 | val_loss=5.8224  val_KL=2.9352  val_NLL_X=2.8872 | lr=0.000040\n",
      "[PVAE ep 49] train_loss=5.7910  train_KL=2.9410  train_NLL_X=2.8500 | val_loss=5.7985  val_KL=2.9576  val_NLL_X=2.8409 | lr=0.000040\n",
      "[PVAE ep 50] train_loss=5.7572  train_KL=2.9393  train_NLL_X=2.8179 | val_loss=5.7831  val_KL=2.9812  val_NLL_X=2.8019 | lr=0.000040\n",
      "[PVAE ep 51] train_loss=5.7406  train_KL=2.9330  train_NLL_X=2.8076 | val_loss=5.7448  val_KL=2.9562  val_NLL_X=2.7885 | lr=0.000040\n",
      "[PVAE ep 52] train_loss=5.7656  train_KL=2.9223  train_NLL_X=2.8433 | val_loss=5.7541  val_KL=3.0203  val_NLL_X=2.7338 | lr=0.000008\n",
      "[PVAE ep 53] train_loss=5.7428  train_KL=2.9348  train_NLL_X=2.8080 | val_loss=5.8516  val_KL=2.9701  val_NLL_X=2.8816 | lr=0.000008\n",
      "[PVAE ep 54] train_loss=5.7558  train_KL=2.9179  train_NLL_X=2.8380 | val_loss=5.8150  val_KL=2.9972  val_NLL_X=2.8178 | lr=0.000008\n",
      "[PVAE ep 55] train_loss=5.7450  train_KL=2.9104  train_NLL_X=2.8347 | val_loss=5.7139  val_KL=2.9003  val_NLL_X=2.8135 | lr=0.000008\n",
      "[PVAE ep 56] train_loss=5.7804  train_KL=2.9349  train_NLL_X=2.8455 | val_loss=5.8208  val_KL=2.9899  val_NLL_X=2.8310 | lr=0.000008\n",
      "[PVAE ep 57] train_loss=5.7313  train_KL=2.9087  train_NLL_X=2.8226 | val_loss=5.7511  val_KL=2.9208  val_NLL_X=2.8302 | lr=0.000008\n",
      "[PVAE ep 58] train_loss=5.7670  train_KL=2.9161  train_NLL_X=2.8509 | val_loss=5.6952  val_KL=2.8910  val_NLL_X=2.8042 | lr=0.000008\n",
      "[PVAE ep 59] train_loss=5.7520  train_KL=2.9157  train_NLL_X=2.8363 | val_loss=5.7907  val_KL=2.9929  val_NLL_X=2.7978 | lr=0.000008\n",
      "[PVAE ep 60] train_loss=5.7383  train_KL=2.9150  train_NLL_X=2.8233 | val_loss=5.7523  val_KL=2.9174  val_NLL_X=2.8349 | lr=0.000008\n",
      "[PVAE ep 61] train_loss=5.7396  train_KL=2.9221  train_NLL_X=2.8175 | val_loss=5.7444  val_KL=2.9235  val_NLL_X=2.8210 | lr=0.000008\n",
      "[PVAE ep 62] train_loss=5.7624  train_KL=2.9458  train_NLL_X=2.8166 | val_loss=5.7695  val_KL=2.9751  val_NLL_X=2.7944 | lr=0.000008\n",
      "[PVAE ep 63] train_loss=5.7556  train_KL=2.9385  train_NLL_X=2.8171 | val_loss=5.7808  val_KL=2.9879  val_NLL_X=2.7929 | lr=0.000008\n",
      "[PVAE ep 64] train_loss=5.7674  train_KL=2.9320  train_NLL_X=2.8354 | val_loss=5.6832  val_KL=2.9343  val_NLL_X=2.7489 | lr=0.000008\n",
      "[PVAE ep 65] train_loss=5.7498  train_KL=2.9119  train_NLL_X=2.8379 | val_loss=5.7330  val_KL=2.9432  val_NLL_X=2.7898 | lr=0.000008\n",
      "[PVAE ep 66] train_loss=5.7606  train_KL=2.9390  train_NLL_X=2.8216 | val_loss=5.8400  val_KL=3.0233  val_NLL_X=2.8166 | lr=0.000008\n",
      "[PVAE ep 67] train_loss=5.7778  train_KL=2.9137  train_NLL_X=2.8642 | val_loss=5.7535  val_KL=2.9490  val_NLL_X=2.8044 | lr=0.000008\n",
      "[PVAE ep 68] train_loss=5.7652  train_KL=2.9392  train_NLL_X=2.8260 | val_loss=5.7626  val_KL=2.9853  val_NLL_X=2.7773 | lr=0.000008\n",
      "[PVAE ep 69] train_loss=5.7755  train_KL=2.9607  train_NLL_X=2.8148 | val_loss=5.8545  val_KL=2.9699  val_NLL_X=2.8846 | lr=0.000008\n",
      "[PVAE ep 70] train_loss=5.7540  train_KL=2.9153  train_NLL_X=2.8387 | val_loss=5.7879  val_KL=2.9401  val_NLL_X=2.8478 | lr=0.000002\n",
      "[PVAE ep 71] train_loss=5.7586  train_KL=2.9316  train_NLL_X=2.8271 | val_loss=5.7637  val_KL=2.9153  val_NLL_X=2.8484 | lr=0.000002\n",
      "[PVAE ep 72] train_loss=5.7598  train_KL=2.9252  train_NLL_X=2.8346 | val_loss=5.7832  val_KL=2.9579  val_NLL_X=2.8252 | lr=0.000002\n",
      "[PVAE ep 73] train_loss=5.7530  train_KL=2.9034  train_NLL_X=2.8496 | val_loss=5.7852  val_KL=2.9472  val_NLL_X=2.8380 | lr=0.000002\n",
      "[PVAE ep 74] train_loss=5.7429  train_KL=2.9251  train_NLL_X=2.8178 | val_loss=5.7156  val_KL=2.9406  val_NLL_X=2.7750 | lr=0.000002\n",
      "[PVAE ep 75] train_loss=5.7711  train_KL=2.9524  train_NLL_X=2.8187 | val_loss=5.7462  val_KL=2.9903  val_NLL_X=2.7559 | lr=0.000002\n",
      "[PVAE ep 76] train_loss=5.7636  train_KL=2.9459  train_NLL_X=2.8177 | val_loss=5.7888  val_KL=2.9981  val_NLL_X=2.7907 | lr=0.000000\n",
      "[PVAE ep 77] train_loss=5.7393  train_KL=2.9139  train_NLL_X=2.8253 | val_loss=5.8271  val_KL=2.9660  val_NLL_X=2.8611 | lr=0.000000\n",
      "[PVAE ep 78] train_loss=5.7687  train_KL=2.9439  train_NLL_X=2.8248 | val_loss=5.8967  val_KL=3.0159  val_NLL_X=2.8808 | lr=0.000000\n",
      "[PVAE ep 79] train_loss=5.7575  train_KL=2.9293  train_NLL_X=2.8281 | val_loss=5.7646  val_KL=2.9222  val_NLL_X=2.8424 | lr=0.000000\n",
      "[PVAE ep 80] train_loss=5.7731  train_KL=2.9482  train_NLL_X=2.8249 | val_loss=5.7868  val_KL=3.0277  val_NLL_X=2.7592 | lr=0.000000\n",
      "[PVAE ep 81] train_loss=5.7630  train_KL=2.9492  train_NLL_X=2.8138 | val_loss=5.8224  val_KL=2.9362  val_NLL_X=2.8862 | lr=0.000000\n",
      "[PVAE ep 82] train_loss=5.7541  train_KL=2.9144  train_NLL_X=2.8398 | val_loss=5.7436  val_KL=2.9550  val_NLL_X=2.7887 | lr=0.000000\n",
      "[PVAE ep 83] train_loss=5.7365  train_KL=2.9115  train_NLL_X=2.8249 | val_loss=5.8432  val_KL=2.9860  val_NLL_X=2.8572 | lr=0.000000\n",
      "[PVAE ep 84] train_loss=5.7764  train_KL=2.9476  train_NLL_X=2.8288 | val_loss=5.8161  val_KL=3.0145  val_NLL_X=2.8016 | lr=0.000000\n",
      "[PVAE ep 85] train_loss=5.7380  train_KL=2.9093  train_NLL_X=2.8287 | val_loss=5.6914  val_KL=2.9550  val_NLL_X=2.7364 | lr=0.000000\n",
      "[PVAE ep 86] train_loss=5.7073  train_KL=2.9043  train_NLL_X=2.8030 | val_loss=5.7772  val_KL=2.9854  val_NLL_X=2.7918 | lr=0.000000\n",
      "[PVAE ep 87] train_loss=5.7538  train_KL=2.9180  train_NLL_X=2.8358 | val_loss=5.6613  val_KL=2.9534  val_NLL_X=2.7080 | lr=0.000000\n",
      "[PVAE ep 88] train_loss=5.7674  train_KL=2.9422  train_NLL_X=2.8252 | val_loss=5.7631  val_KL=2.9247  val_NLL_X=2.8384 | lr=0.000000\n",
      "[PVAE ep 89] train_loss=5.7468  train_KL=2.9243  train_NLL_X=2.8225 | val_loss=5.7840  val_KL=2.9665  val_NLL_X=2.8175 | lr=0.000000\n",
      "[PVAE ep 90] train_loss=5.7626  train_KL=2.9397  train_NLL_X=2.8229 | val_loss=5.7091  val_KL=2.9627  val_NLL_X=2.7464 | lr=0.000000\n",
      "[PVAE ep 91] train_loss=5.7582  train_KL=2.9097  train_NLL_X=2.8486 | val_loss=5.7438  val_KL=2.9768  val_NLL_X=2.7670 | lr=0.000000\n",
      "[PVAE ep 92] train_loss=5.7526  train_KL=2.9281  train_NLL_X=2.8245 | val_loss=5.7174  val_KL=2.9227  val_NLL_X=2.7947 | lr=0.000000\n",
      "[PVAE ep 93] train_loss=5.7595  train_KL=2.9341  train_NLL_X=2.8254 | val_loss=5.7782  val_KL=3.0132  val_NLL_X=2.7650 | lr=0.000000\n",
      "[PVAE ep 94] train_loss=5.7310  train_KL=2.9121  train_NLL_X=2.8189 | val_loss=5.8207  val_KL=2.9722  val_NLL_X=2.8485 | lr=0.000000\n",
      "[PVAE ep 95] train_loss=5.7310  train_KL=2.9035  train_NLL_X=2.8275 | val_loss=5.8066  val_KL=2.9578  val_NLL_X=2.8488 | lr=0.000000\n",
      "[PVAE ep 96] train_loss=5.7811  train_KL=2.9258  train_NLL_X=2.8553 | val_loss=5.6947  val_KL=2.9200  val_NLL_X=2.7747 | lr=0.000000\n",
      "[PVAE ep 97] train_loss=5.7258  train_KL=2.9316  train_NLL_X=2.7942 | val_loss=5.7917  val_KL=2.9354  val_NLL_X=2.8562 | lr=0.000000\n",
      "[PVAE ep 98] train_loss=5.7641  train_KL=2.9175  train_NLL_X=2.8466 | val_loss=5.7629  val_KL=2.9662  val_NLL_X=2.7968 | lr=0.000000\n",
      "[PVAE ep 99] train_loss=5.7677  train_KL=2.9394  train_NLL_X=2.8283 | val_loss=5.7490  val_KL=2.9443  val_NLL_X=2.8047 | lr=0.000000\n",
      "Best val_loss = 5.6613\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "pvae = PartialVAE(\n",
    "    input_type=\"continuous\",\n",
    "    num_con_features=8,\n",
    "    num_cat_features=0,\n",
    "    hidden_dim_con=30,\n",
    "    most_categories=max(1, 0),   # 내부 차원 계산을 위해 최소 1\n",
    "    c_dim=16,\n",
    "    hid_enc=100,\n",
    "    hid_dec=100,\n",
    "    latent_dim=30\n",
    ")\n",
    "optimizer = AdamW(pvae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_generator(\n",
    "    generator=pvae,\n",
    "    train_loader=train_loader,\n",
    "    X_val=X_val,\n",
    "    D=D, # feature 개수\n",
    "    epochs=epochs,\n",
    "    optimizer=optimizer,\n",
    "    obs_sigma=0.2,\n",
    "    lr_factor=0.2,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7,\n",
    "    scheduler_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0258bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/8 | Acc: 0.4782\n",
      "Step 2/8 | Acc: 0.5068\n",
      "Step 3/8 | Acc: 0.5557\n",
      "Step 4/8 | Acc: 0.5577\n",
      "Step 5/8 | Acc: 0.5799\n",
      "Step 6/8 | Acc: 0.5950\n",
      "Step 7/8 | Acc: 0.6114\n",
      "Step 8/8 | Acc: 0.6361\n"
     ]
    }
   ],
   "source": [
    "predictor.eval()\n",
    "N = X_test.size(0)\n",
    "D = X_test.size(1)\n",
    "x_np = X_test.cpu().numpy()\n",
    "m_np = np.zeros((N, D), dtype=np.float32)\n",
    "\n",
    "result = []\n",
    "\n",
    "accs = run_feature_acquisition(\n",
    "    predictor=predictor,\n",
    "    generator=pvae,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    x_np=x_np,\n",
    "    m_np=m_np,\n",
    "    D=D,\n",
    "    num_samples=10,\n",
    "    alpha=1,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "result.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d15243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 평균: 0.5651041\n",
      "표준편차: 0.0\n",
      "acquisition 별 평균): [0.47819766 0.50678295 0.55571705 0.55765504 0.57994187 0.5949612\n",
      " 0.6114341  0.6361434 ]\n",
      "Mean ACC over all acquisitions = 0.5651\n"
     ]
    }
   ],
   "source": [
    "result = np.array(result, dtype=np.float32)\n",
    "\n",
    "num_runs, D = result.shape\n",
    "row_means = result.mean(axis=1)\n",
    "overall_mean = row_means.mean()\n",
    "\n",
    "std = row_means.std()\n",
    "\n",
    "col_means = result.mean(axis=0)\n",
    "\n",
    "print(\"전체 평균:\", overall_mean)\n",
    "print(\"표준편차:\", std)\n",
    "print(\"acquisition 별 평균):\", col_means)\n",
    "\n",
    "# 시각화\n",
    "xs = np.arange(1, D + 1)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(xs, col_means, marker='o')\n",
    "plt.xlabel('Acquisition No.', fontsize=11)\n",
    "plt.ylabel('ACCURACY', fontsize=11)\n",
    "plt.title('California Housing', fontsize=12)\n",
    "plt.xticks(np.arange(0, D+1, 2))\n",
    "plt.xlim(0, D + 1)\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 확대 영역\n",
    "ax = plt.gca()\n",
    "axins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='center right')\n",
    "\n",
    "axins.plot(xs, col_means, marker='o')\n",
    "axins.set_xlim(6, 8)\n",
    "axins.set_ylim(0.92, 0.98)\n",
    "axins.grid(True, alpha=0.3)\n",
    "axins.tick_params(labelsize=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "mean_acc = np.nanmean(col_means)\n",
    "print(f\"Mean ACC over all acquisitions = {mean_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1188da",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d595cfa",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6404552",
   "metadata": {},
   "source": [
    "# MiniBooNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcba30",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597dafdf",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a75fb4",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b308b",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b0c4d",
   "metadata": {},
   "source": [
    "# Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36227928",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3f940",
   "metadata": {},
   "source": [
    "# METABRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab47532",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397befb7",
   "metadata": {},
   "source": [
    "# TCGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b20bbb",
   "metadata": {},
   "source": [
    "## Partial VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACFlow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
